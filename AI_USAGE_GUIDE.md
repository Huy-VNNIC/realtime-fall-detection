# ü§ñ AI Features Guide - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng AI

## üìã M·ª•c l·ª•c

1. [T·ªïng quan](#t·ªïng-quan)
2. [C√†i ƒë·∫∑t nhanh](#c√†i-ƒë·∫∑t-nhanh)
3. [C√°c ch·ª©c nƒÉng AI](#c√°c-ch·ª©c-nƒÉng-ai)
4. [Training Models](#training-models)
5. [N√¢ng cao](#n√¢ng-cao)
6. [Troubleshooting](#troubleshooting)

---

## üéØ T·ªïng quan

### H·ªá th·ªëng AI hi·ªán t·∫°i c·ªßa b·∫°n:

```
‚úÖ OpenCV Detection      - ƒêang ho·∫°t ƒë·ªông
‚úÖ Feature Extraction    - ƒêang ho·∫°t ƒë·ªông  
‚ö†Ô∏è ML Classifier         - C·∫ßn train model
‚öôÔ∏è XGBoost              - C·∫ßn c√†i & train
‚öôÔ∏è Deep Learning        - C·∫ßn c√†i PyTorch
‚öôÔ∏è Pose Estimation      - C·∫ßn c√†i MediaPipe
```

---

## üöÄ C√†i ƒë·∫∑t nhanh

### B∆∞·ªõc 1: C√†i ƒë·∫∑t packages c∆° b·∫£n (ƒê√É XONG)

```bash
pip install opencv-python numpy scipy scikit-learn pyyaml
```

‚úÖ B·∫°n ƒë√£ c√†i xong r·ªìi!

### B∆∞·ªõc 2: C√†i ƒë·∫∑t AI n√¢ng cao (T√ôY CH·ªåN)

```bash
# XGBoost - TƒÉng accuracy l√™n 90%
pip install xgboost

# MediaPipe - Pose detection (r·∫•t ch√≠nh x√°c)
pip install mediapipe

# Deep Learning - Accuracy 95%+ (n·∫∑ng)
pip install torch torchvision

# Explainable AI
pip install shap
```

### B∆∞·ªõc 3: Ki·ªÉm tra c√†i ƒë·∫∑t

```bash
python demo_ai_features.py
```

---

## üß† C√°c ch·ª©c nƒÉng AI

### 1. ML Classifier (Random Forest)

**Hi·ªán tr·∫°ng:** ‚ö†Ô∏è Ch∆∞a train  
**ƒê·ªô ch√≠nh x√°c:** ~85%  
**T·ªëc ƒë·ªô:** 15ms/frame  

**C√°ch s·ª≠ d·ª•ng:**

```bash
# Thu th·∫≠p data (5-10 ph√∫t)
cd data
python collector.py --mode fall --duration 60
python collector.py --mode not_fall --duration 120

# Train model (30 gi√¢y)
python train.py

# K√≠ch ho·∫°t trong config.yaml
ml_classifier:
  enabled: true
```

**L·ª£i √≠ch:**
- ‚úÖ Gi·∫£m false alarm t·ª´ 30% ‚Üí 12%
- ‚úÖ H·ªçc pattern th·ª±c t·∫ø t·ª´ m√¥i tr∆∞·ªùng c·ªßa b·∫°n
- ‚úÖ Nhanh, nh·∫π, ch·∫°y ƒë∆∞·ª£c tr√™n m·ªçi m√°y

---

### 2. XGBoost Classifier (N√¢ng cao)

**Hi·ªán tr·∫°ng:** ‚öôÔ∏è C·∫ßn c√†i & train  
**ƒê·ªô ch√≠nh x√°c:** ~90%  
**T·ªëc ƒë·ªô:** 12ms/frame  

**C√°ch s·ª≠ d·ª•ng:**

```bash
# C√†i XGBoost
pip install xgboost

# Train v·ªõi XGBoost
cd data
python train_advanced.py --optimize

# K√≠ch ho·∫°t
# Edit config.yaml:
ml_classifier:
  use_xgboost: true
```

**L·ª£i √≠ch:**
- ‚úÖ Accuracy cao h∆°n Random Forest 5%
- ‚úÖ Nhanh h∆°n Random Forest
- ‚úÖ Feature importance (bi·∫øt feature n√†o quan tr·ªçng)
- ‚úÖ SHAP explanation (gi·∫£i th√≠ch t·∫°i sao alarm)

**Khi n√†o d√πng:**
- Khi c·∫ßn accuracy cao nh·∫•t m√† v·∫´n nhanh
- Production environment
- Nhi·ªÅu data (>1000 samples)

---

### 3. Pose Estimation (MediaPipe)

**Hi·ªán tr·∫°ng:** ‚öôÔ∏è C·∫ßn c√†i MediaPipe  
**ƒê·ªô ch√≠nh x√°c:** ~92%  
**T·ªëc ƒë·ªô:** 30ms/frame  

**C√°ch s·ª≠ d·ª•ng:**

```bash
# C√†i MediaPipe
pip install mediapipe

# K√≠ch ho·∫°t
# Edit config.yaml:
deep_learning:
  pose_estimation:
    enabled: true
```

**L·ª£i √≠ch:**
- ‚úÖ Ph√°t hi·ªán ch√≠nh x√°c h∆°n (d√πng body keypoints)
- ‚úÖ Explainable (bi·∫øt ch√≠nh x√°c t∆∞ th·∫ø c∆° th·ªÉ)
- ‚úÖ √çt b·ªã false alarm do v·∫≠t th·ªÉ kh√°c
- ‚úÖ Detect ƒë∆∞·ª£c nhi·ªÅu ki·ªÉu ng√£ ph·ª©c t·∫°p

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå Ch·∫≠m h∆°n (30ms vs 15ms)
- ‚ùå C·∫ßn CPU/GPU m·∫°nh

**Khi n√†o d√πng:**
- Khi accuracy quan tr·ªçng h∆°n speed
- M√¥i tr∆∞·ªùng c√≥ nhi·ªÅu v·∫≠t th·ªÉ g√¢y nhi·ªÖu
- C·∫ßn gi·∫£i th√≠ch r√µ t·∫°i sao alarm

---

### 4. Deep Learning (CNN-LSTM)

**Hi·ªán tr·∫°ng:** üöÄ Future feature  
**ƒê·ªô ch√≠nh x√°c:** ~95%+  
**T·ªëc ƒë·ªô:** 50ms/frame  

**C√°ch s·ª≠ d·ª•ng:**

```bash
# C√†i PyTorch
pip install torch torchvision

# Code ƒë√£ c√≥ s·∫µn trong ai/deep_learning.py
# C·∫ßn: 
#   - Collect nhi·ªÅu data (>10,000 samples)
#   - Train model (c·∫ßn GPU, 1-2 gi·ªù)
#   - Deploy model
```

**L·ª£i √≠ch:**
- ‚úÖ Accuracy cao nh·∫•t
- ‚úÖ H·ªçc ƒë∆∞·ª£c temporal patterns ph·ª©c t·∫°p
- ‚úÖ Generalize t·ªët v·ªõi data m·ªõi

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ùå C·∫ßn nhi·ªÅu data
- ‚ùå C·∫ßn GPU ƒë·ªÉ train & inference
- ‚ùå Ph·ª©c t·∫°p ƒë·ªÉ deploy

**Khi n√†o d√πng:**
- Research/academic projects
- Khi c√≥ GPU v√† nhi·ªÅu data
- Khi c·∫ßn accuracy tuy·ªát ƒë·ªëi

---

### 5. Online Learning

**Hi·ªán tr·∫°ng:** ‚úÖ Code c√≥ s·∫µn  
**Ch·ª©c nƒÉng:** H·ªçc li√™n t·ª•c t·ª´ feedback  

**C√°ch s·ª≠ d·ª•ng:**

```bash
# K√≠ch ho·∫°t trong config.yaml
ml_classifier:
  online_learning:
    enabled: true
    update_interval: 100
```

**Workflow:**

```
1. H·ªá th·ªëng ph√°t hi·ªán ng√£ ‚Üí ALARM
2. B·∫°n x√°c nh·∫≠n: "ƒê√∫ng l√† ng√£" ho·∫∑c "Kh√¥ng ph·∫£i ng√£"
3. Model t·ª± ƒë·ªông h·ªçc t·ª´ feedback
4. L·∫ßn sau ch√≠nh x√°c h∆°n!
```

**L·ª£i √≠ch:**
- ‚úÖ Model t·ª± ƒë·ªông c·∫£i thi·ªán theo th·ªùi gian
- ‚úÖ Adapt v·ªõi m√¥i tr∆∞·ªùng c·ª• th·ªÉ c·ªßa b·∫°n
- ‚úÖ Kh√¥ng c·∫ßn retrain manually

---

### 6. Ensemble Detection

**Hi·ªán tr·∫°ng:** ‚úÖ Code c√≥ s·∫µn  
**ƒê·ªô ch√≠nh x√°c:** ~92%  
**T·ªëc ƒë·ªô:** 25ms/frame  

**C√°ch s·ª≠ d·ª•ng:**

```bash
# Train nhi·ªÅu models
cd data
python train_advanced.py  # Train c·∫£ RF v√† XGBoost

# K√≠ch ho·∫°t ensemble
# Edit config.yaml:
ml_classifier:
  ensemble:
    enabled: true
    models: ['ml', 'xgboost', 'pose']
    voting: 'weighted'
```

**C√°ch ho·∫°t ƒë·ªông:**

```
Frame ‚Üí [Model 1 (40%)] ‚Üí 0.8 fall
      ‚Üí [Model 2 (30%)] ‚Üí 0.9 fall  
      ‚Üí [Model 3 (30%)] ‚Üí 0.7 fall
      
Ensemble = 0.8*0.4 + 0.9*0.3 + 0.7*0.3 = 0.81 fall
```

**L·ª£i √≠ch:**
- ‚úÖ Accuracy cao h∆°n model ƒë∆°n l·∫ª
- ‚úÖ Robust v·ªõi edge cases
- ‚úÖ Confidence scores ƒë√°ng tin h∆°n

---

## üìö Training Models

### Quick Training (5 ph√∫t)

```bash
# B∆∞·ªõc 1: Thu data ng√£ (1 ph√∫t)
cd data
python collector.py --mode fall --duration 60
# ‚Üí L√†m: ng√£ xu·ªëng, n·∫±m, rolling

# B∆∞·ªõc 2: Thu data kh√¥ng ng√£ (2 ph√∫t)  
python collector.py --mode not_fall --duration 120
# ‚Üí L√†m: ƒëi l·∫°i, ng·ªìi, ƒë·ª©ng, c√∫i nh·∫∑t ƒë·ªì

# B∆∞·ªõc 3: Train (30 gi√¢y)
python train.py

# B∆∞·ªõc 4: Test
cd ..
python main.py
```

### Advanced Training

```bash
# Train t·∫•t c·∫£ models + optimize
cd data
python train_advanced.py --optimize

# Results:
# - Random Forest model
# - XGBoost model
# - Performance comparison
# - Feature importance chart
# - ROC curves
# - Confusion matrix
```

### Tips ƒë·ªÉ collect data t·ªët:

1. **Balance classes:**
   - 50% fall, 50% not-fall
   - T·ªëi thi·ªÉu: 100 samples m·ªói class
   - Khuy·∫øn ngh·ªã: 500+ samples m·ªói class

2. **Diverse scenarios:**
   - Nhi·ªÅu ki·ªÉu ng√£: th·∫≥ng xu·ªëng, nghi√™ng, t·ª´ gh·∫ø
   - Nhi·ªÅu ng∆∞·ªùi: cao th·∫•p b√©o g·∫ßy
   - Nhi·ªÅu ƒëi·ªÅu ki·ªán √°nh s√°ng

3. **Edge cases:**
   - Ng·ªìi xu·ªëng nhanh (kh√¥ng ph·∫£i ng√£!)
   - C√∫i xu·ªëng nh·∫∑t ƒë·ªì
   - Nh·∫£y m√∫a, yoga
   - N·∫±m ng·ªß

4. **Negative samples quan tr·ªçng:**
   - Collect 2x data not-fall so v·ªõi fall
   - Gi√∫p gi·∫£m false alarms

---

## üîß N√¢ng cao

### 1. Hyperparameter Tuning

```bash
# Auto-optimize t·∫•t c·∫£ parameters
cd data
python train_advanced.py --optimize

# Ho·∫∑c manual tuning trong code:
# Edit train_advanced.py:
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.3],
    'n_estimators': [50, 100, 200, 500],
}
```

### 2. Feature Engineering

Th√™m features m·ªõi trong `ai/feature_extractor.py`:

```python
# Example: Add optical flow feature
def extract_optical_flow(self, frame, prev_frame):
    flow = cv2.calcOpticalFlowFarneback(
        prev_frame, frame, None,
        0.5, 3, 15, 3, 5, 1.2, 0
    )
    magnitude = np.sqrt(flow[...,0]**2 + flow[...,1]**2)
    return {
        'flow_mean': np.mean(magnitude),
        'flow_max': np.max(magnitude)
    }
```

### 3. Model Export

```bash
# Export to ONNX (for deployment)
# Edit train_advanced.py, add:
import onnx
import skl2onnx

onnx_model = skl2onnx.convert_sklearn(model, initial_types=[...])
with open('model.onnx', 'wb') as f:
    f.write(onnx_model.SerializeToString())
```

### 4. A/B Testing

```python
# Test new model vs old model
# config.yaml:
ml_classifier:
  ab_testing:
    enabled: true
    champion_model: "xgboost_v1.pkl"
    challenger_model: "xgboost_v2.pkl"
    traffic_split: 0.5  # 50% each
```

---

## üêõ Troubleshooting

### Issue 1: Model kh√¥ng load

**L·ªói:** `Model not found at ai/models/fall_classifier.pkl`

**Gi·∫£i ph√°p:**
```bash
cd data
python train.py  # Train model tr∆∞·ªõc
```

---

### Issue 2: Accuracy th·∫•p

**Nguy√™n nh√¢n:**
- Kh√¥ng ƒë·ªß data
- Data kh√¥ng diverse
- Class imbalance

**Gi·∫£i ph√°p:**
```bash
# Thu th√™m data
python collector.py --mode fall --duration 300
python collector.py --mode not_fall --duration 600

# Balance classes
# ƒê·∫£m b·∫£o: not_fall samples = 2x fall samples
```

---

### Issue 3: Qu√° nhi·ªÅu false alarms

**Gi·∫£i ph√°p 1:** TƒÉng confidence threshold
```yaml
# config.yaml
ml_classifier:
  confidence_threshold: 0.85  # TƒÉng t·ª´ 0.7 ‚Üí 0.85
```

**Gi·∫£i ph√°p 2:** Collect more negative samples
```bash
# Thu data c√°c tr∆∞·ªùng h·ª£p b·ªã nh·∫ßm
python collector.py --mode not_fall --duration 300
# L√†m: ng·ªìi nhanh, c√∫i xu·ªëng, v.v.
```

**Gi·∫£i ph√°p 3:** D√πng Ensemble
```yaml
ml_classifier:
  ensemble:
    enabled: true
```

---

### Issue 4: Ch·∫≠m (Low FPS)

**Hi·ªán t·∫°i:** OpenCV only ‚Üí 30-60 FPS  
**V·ªõi ML:** Random Forest ‚Üí 20-40 FPS  
**V·ªõi XGBoost:** ‚Üí 25-45 FPS  
**V·ªõi Pose:** ‚Üí 10-20 FPS  

**T·ªëi ∆∞u:**

1. **Gi·∫£m resolution:**
```yaml
camera:
  width: 480  # T·ª´ 640 ‚Üí 480
  height: 360
```

2. **Skip frames:**
```python
# Process m·ªói 2 frames
if frame_count % 2 == 0:
    prediction = classifier.predict(features)
```

3. **Use GPU (if available):**
```yaml
deep_learning:
  use_gpu: true
```

---

## üìä Performance Metrics

### Hi·ªán t·∫°i (OpenCV only):

| Metric | Value |
|--------|-------|
| Accuracy | 70% |
| Precision | 75% |
| Recall | 65% |
| F1 Score | 70% |
| False Positive Rate | 30% |
| FPS | 50+ |

### Target (With AI):

| Model | Accuracy | FP Rate | FPS |
|-------|----------|---------|-----|
| Random Forest | 85% | 12% | 30+ |
| XGBoost | 90% | 8% | 35+ |
| Ensemble | 92% | 6% | 25+ |
| Deep Learning | 95%+ | <5% | 15+ |

---

## üéì Best Practices

### 1. Development Flow

```
1. Start v·ªõi OpenCV (baseline)
2. Collect data (100+ samples)
3. Train Random Forest
4. Evaluate & tune
5. If needed: XGBoost
6. If needed: Ensemble
7. Production deploy
```

### 2. Data Collection Strategy

```
Week 1: Basic scenarios (100 samples)
Week 2: Edge cases (200 samples)
Week 3: Different people (300 samples)
Week 4: Production testing
```

### 3. Model Versioning

```
models/
  ‚îú‚îÄ‚îÄ v1_baseline_rf.pkl
  ‚îú‚îÄ‚îÄ v2_tuned_rf.pkl
  ‚îú‚îÄ‚îÄ v3_xgboost.pkl
  ‚îî‚îÄ‚îÄ production/
      ‚îî‚îÄ‚îÄ current_model.pkl
```

### 4. Monitoring

```python
# Log predictions for analysis
logger.log_prediction({
    'timestamp': time.time(),
    'prediction': result['class'],
    'confidence': result['proba'],
    'features': features.tolist(),
    'ground_truth': None  # Fill later
})
```

---

## üöÄ Roadmap

### Phase 1: Basic ML (Tu·∫ßn n√†y)
- [x] Feature extraction
- [x] Random Forest classifier
- [ ] **‚Üí Train first model** ‚Üê B·∫†N ·ªû ƒê√ÇY
- [ ] Test & validate

### Phase 2: Advanced ML (Tu·∫ßn sau)
- [ ] XGBoost integration
- [ ] Online learning
- [ ] Ensemble methods

### Phase 3: Deep Learning (Th√°ng sau)
- [ ] Pose estimation (MediaPipe)
- [ ] CNN-LSTM model
- [ ] Transformer model

### Phase 4: Production (2-3 th√°ng)
- [ ] Edge deployment (Raspberry Pi)
- [ ] Model monitoring
- [ ] A/B testing
- [ ] Auto-retraining

---

## üìû Support

### C·∫ßn gi√∫p ƒë·ª°?

1. **Check demo:**
   ```bash
   python demo_ai_features.py
   ```

2. **Xem report:**
   ```bash
   cat AI_SYSTEM_REPORT.md
   ```

3. **Test installation:**
   ```bash
   python test_installation.py
   ```

---

## üéØ Quick Start Checklist

- [x] ‚úÖ H·ªá th·ªëng ƒë√£ ch·∫°y
- [x] ‚úÖ OpenCV detection ho·∫°t ƒë·ªông
- [ ] ‚è≥ Collect training data (5 ph√∫t)
- [ ] ‚è≥ Train first model (30 gi√¢y)
- [ ] ‚è≥ Test AI detection
- [ ] ‚è≥ Fine-tune thresholds
- [ ] üéâ Production ready!

---

**B·∫°n ƒëang ·ªü:** ‚è≥ B∆∞·ªõc 3 - C·∫ßn collect data & train model

**Next action:** 
```bash
cd data
python collector.py --mode fall --duration 60
```

Ch√∫c may m·∫Øn! üöÄ
